{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fPjjUZHwFgSM"
   },
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 472,
     "status": "ok",
     "timestamp": 1615410579003,
     "user": {
      "displayName": "Chuan Chen",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjGNZ_BC0raiaLxFCSgR1dppBgMgjY4TPSe81Bz=s64",
      "userId": "01488084844237739692"
     },
     "user_tz": 300
    },
    "id": "QuZ4G8DmFhOJ"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"   #if like me you do not have a lot of memory in your GPU\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"\" #then these two lines force keras to use your CPU\n",
    "import tensorflow.keras as keras\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Flatten, Conv3D, MaxPooling3D, Dropout, BatchNormalization, LeakyReLU, Conv2DTranspose, ReLU, Reshape\n",
    "# from keras.models import Sequential\n",
    "# from keras.layers import Dense, \n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "import random\n",
    "import itertools\n",
    "import glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "import sparse\n",
    "  # not scipy sparse because that is not how michael encoded it"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defocus parameters\n",
    "Carina: Here are the defocus parameters, ordered the same as X and y. You could pull it into the Data Generator easily enough, but I'm not sure in Keras how to pass it in in the middle of the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defocus params\n",
    "with open(r\"../em_data/defocus_list.pkl\", 'rb') as f:\n",
    "  defocus_list = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-cZn30ncTHPc"
   },
   "source": [
    "# load data - small size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 1259,
     "status": "ok",
     "timestamp": 1615399361091,
     "user": {
      "displayName": "Chuan Chen",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjGNZ_BC0raiaLxFCSgR1dppBgMgjY4TPSe81Bz=s64",
      "userId": "01488084844237739692"
     },
     "user_tz": 300
    },
    "id": "dmuOuuF7THPc"
   },
   "outputs": [],
   "source": [
    "with open(r\"../em_data/X_list_84x54x49.pkl\", 'rb') as f:\n",
    "  # despite the name, it has dimensions 84x54x98\n",
    "  X_3d = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 2156,
     "status": "ok",
     "timestamp": 1615399368492,
     "user": {
      "displayName": "Chuan Chen",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjGNZ_BC0raiaLxFCSgR1dppBgMgjY4TPSe81Bz=s64",
      "userId": "01488084844237739692"
     },
     "user_tz": 300
    },
    "id": "bwyRxc58THPc"
   },
   "outputs": [],
   "source": [
    "with open(r\"../em_data/y_list_84x54.pkl\", 'rb') as f:\n",
    "  y_2d = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 417,
     "status": "ok",
     "timestamp": 1615400403139,
     "user": {
      "displayName": "Chuan Chen",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjGNZ_BC0raiaLxFCSgR1dppBgMgjY4TPSe81Bz=s64",
      "userId": "01488084844237739692"
     },
     "user_tz": 300
    },
    "id": "CFrjsJLpTHPd",
    "outputId": "af805fee-3734-4c58-d479-98586580564f"
   },
   "outputs": [],
   "source": [
    "print(len(X_3d))\n",
    "print(type(X_3d))\n",
    "print(X_3d[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 406,
     "status": "ok",
     "timestamp": 1615400412936,
     "user": {
      "displayName": "Chuan Chen",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjGNZ_BC0raiaLxFCSgR1dppBgMgjY4TPSe81Bz=s64",
      "userId": "01488084844237739692"
     },
     "user_tz": 300
    },
    "id": "sqE8Cu4kTHPd",
    "outputId": "af9d0188-1a73-4dde-82cd-59162086eef9"
   },
   "outputs": [],
   "source": [
    "print(len(y_2d))\n",
    "print(type(y_2d))\n",
    "print(y_2d[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 1498,
     "status": "ok",
     "timestamp": 1615399430552,
     "user": {
      "displayName": "Chuan Chen",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjGNZ_BC0raiaLxFCSgR1dppBgMgjY4TPSe81Bz=s64",
      "userId": "01488084844237739692"
     },
     "user_tz": 300
    },
    "id": "X2wNASOXTHPd"
   },
   "outputs": [],
   "source": [
    "# every element in the X_3d list is a sparse matrix, convert it to a 3D numpy array\n",
    "# takes too large of a memory, only process the first 3 matrix\n",
    "X_array = []\n",
    "for i in range(3):\n",
    "  X_array.append(sparse.COO.todense(X_3d[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 461,
     "status": "ok",
     "timestamp": 1615410136723,
     "user": {
      "displayName": "Chuan Chen",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjGNZ_BC0raiaLxFCSgR1dppBgMgjY4TPSe81Bz=s64",
      "userId": "01488084844237739692"
     },
     "user_tz": 300
    },
    "id": "pO2iPXQ2THPe"
   },
   "outputs": [],
   "source": [
    "# change X to be an array of input shape (#_samples, x, y, z)\n",
    "# change y to be an array of output shape (#_samples, x, y)\n",
    "# also crop them to be squares/cubes\n",
    "# but for the 3D, do not take the canter on the z-dim, take the front because those are closer to the camera\n",
    "\n",
    "X_train = np.zeros(shape=(3, 32, 32, 32,1))\n",
    "y_train = np.zeros(shape=(3,32,32,1))\n",
    "  # must add a channel dimension even if we only have 1 channel\n",
    "  # the keras model require the extra channel dimension to do 2d and 3d convolutions\n",
    "\n",
    "for i in range(3):\n",
    "  X_train[i, :, :, :, 0] = X_array[i][26:58, 11:43, 0:32]\n",
    "\n",
    "for i in range(3):\n",
    "  y_train[i, :, :, 0]  = y_2d[i][11:43, 26:58]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 525,
     "status": "ok",
     "timestamp": 1615410138050,
     "user": {
      "displayName": "Chuan Chen",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjGNZ_BC0raiaLxFCSgR1dppBgMgjY4TPSe81Bz=s64",
      "userId": "01488084844237739692"
     },
     "user_tz": 300
    },
    "id": "5Y6HUAeWTHPe",
    "outputId": "33b0efe8-1cea-428b-b511-4b06acd90bce"
   },
   "outputs": [],
   "source": [
    "np.unique(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 580,
     "status": "ok",
     "timestamp": 1615410139997,
     "user": {
      "displayName": "Chuan Chen",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjGNZ_BC0raiaLxFCSgR1dppBgMgjY4TPSe81Bz=s64",
      "userId": "01488084844237739692"
     },
     "user_tz": 300
    },
    "id": "5-b1BAxIWdst",
    "outputId": "a1f883ea-3f17-473b-86bb-4277bb2b42d2"
   },
   "outputs": [],
   "source": [
    "print(X_train.shape)\n",
    "print(y_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DataGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataGenerator(keras.utils.Sequence):\n",
    "    'Generates data for Keras'\n",
    "    def __init__(self, X_filename, y_filename, use_range, voxel_dim, img_dim, batch_size, shuffle=True):\n",
    "        self.X_filename = X_filename\n",
    "        self.y_filename = y_filename\n",
    "        self.use_range = use_range\n",
    "        self.voxel_dim = voxel_dim\n",
    "        self.img_dim = img_dim\n",
    "        self.shuffle = shuffle # Not implemented yet\n",
    "        self.batch_size = batch_size\n",
    "        self.X = self.load_pickle(X_filename) # List of arrays\n",
    "        self.y = self.load_pickle(y_filename) # List of arrays\n",
    "\n",
    "    def load_pickle(self,filename):\n",
    "        with open(filename, 'rb') as f:\n",
    "            data = pickle.load(f)\n",
    "        first_idx = int(len(data)*self.use_range[0])\n",
    "        last_idx =  int(len(data)*self.use_range[1])\n",
    "        return data[first_idx:last_idx]\n",
    "\n",
    "    def __len__(self):\n",
    "        'Denotes the number of batches per epoch'\n",
    "        sample_count = len(self.X)\n",
    "        return int(sample_count / self.batch_size)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        'Generate one batch of data'\n",
    "        X = np.asarray([sparse.COO.todense(xi) for xi in self.X[index*self.batch_size:(index+1)*self.batch_size]])\n",
    "        y = np.asarray(self.y[index*self.batch_size:(index+1)*self.batch_size])\n",
    "        X = np.expand_dims(X[:,26:58, 11:43, 0:32],axis=-1) # Generalize this\n",
    "        y = np.expand_dims(y[:,11:43, 26:58],axis=-1) # Generalize this\n",
    "        # Adjust y to [0,1]\n",
    "        y_train = y[:int(y.shape[0]*0.8)]\n",
    "        y_adj = (y-y_train.min()) / (y_train.max()-y_train.min())\n",
    "        return X, y_adj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_filename = '../em_data/X_list_84x54x49.pkl'\n",
    "y_filename = '../em_data/y_list_84x54.pkl'\n",
    "train_range = [0.,0.8]\n",
    "valid_range = [0.8,0.9]\n",
    "test_range = [0.9,1.]\n",
    "train_generator = DataGenerator(X_filename,y_filename,train_range,32,32,5)\n",
    "valid_generator = DataGenerator(X_filename,y_filename,valid_range,32,32,5)\n",
    "test_generator = DataGenerator(X_filename,y_filename,test_range,32,32,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X,y = train_generator[22]\n",
    "print(X.shape)\n",
    "print(y.shape)\n",
    "print(y.min(),y.max())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Lioa3qsOXHSB"
   },
   "source": [
    "# model setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 566,
     "status": "ok",
     "timestamp": 1615406242577,
     "user": {
      "displayName": "Chuan Chen",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjGNZ_BC0raiaLxFCSgR1dppBgMgjY4TPSe81Bz=s64",
      "userId": "01488084844237739692"
     },
     "user_tz": 300
    },
    "id": "wGNxV2wMtB01"
   },
   "outputs": [],
   "source": [
    " sample_shape = (32, 32, 32,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1753,
     "status": "ok",
     "timestamp": 1615411201661,
     "user": {
      "displayName": "Chuan Chen",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjGNZ_BC0raiaLxFCSgR1dppBgMgjY4TPSe81Bz=s64",
      "userId": "01488084844237739692"
     },
     "user_tz": 300
    },
    "id": "LuuQ_5x_uD8m",
    "outputId": "03e2d199-9a07-4f0b-96af-9a22aa380655",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Create the model\n",
    "model = Sequential()\n",
    "model.add(Conv3D(filters=32, \n",
    "                 kernel_size=(4, 4, 4), \n",
    "                 strides=(2, 2, 2),\n",
    "                 padding='same', \n",
    "                    # `same` just means as long as even just the left most 1 column of your kernel is still in the sample matrix, you will use padding to fill the parts that ran over the matrix and finish that mapping\n",
    "                    # if you keep moving till you kernel does not overlap with your matrix at all we will stop and won't pad\n",
    "                 use_bias=False,\n",
    "                 input_shape=sample_shape))\n",
    "# model.add(MaxPooling3D(pool_size=(2, 2, 2)))\n",
    "model.add(BatchNormalization(center=True, scale=True))\n",
    "model.add(LeakyReLU(alpha=0.2))\n",
    "\n",
    "model.add(Conv3D(filters=64, \n",
    "                 kernel_size=(4, 4, 4), \n",
    "                 strides=(2, 2, 2),\n",
    "                 padding='same',\n",
    "                 use_bias=False))\n",
    "model.add(BatchNormalization(center=True, scale=True))\n",
    "model.add(LeakyReLU(alpha=0.2))\n",
    "\n",
    "model.add(Conv3D(filters=128, \n",
    "                 kernel_size=(4, 4, 4), \n",
    "                 strides=(2, 2, 2),\n",
    "                 padding='same',\n",
    "                 use_bias=False))\n",
    "model.add(BatchNormalization(center=True, scale=True))\n",
    "model.add(LeakyReLU(alpha=0.2))\n",
    "\n",
    "model.add(Conv3D(filters=256, \n",
    "                 kernel_size=(4, 4, 4), \n",
    "                 strides=(2, 2, 2),\n",
    "                 padding='same',\n",
    "                 use_bias=False))\n",
    "model.add(BatchNormalization(center=True, scale=True))\n",
    "model.add(LeakyReLU(alpha=0.2))\n",
    "\n",
    "model.add(Conv3D(filters=100, \n",
    "                 kernel_size=(2, 2, 2), \n",
    "                 strides=(1, 1, 1),\n",
    "                 padding='valid',\n",
    "                 use_bias=False))\n",
    "model.add(LeakyReLU(alpha=0.2))\n",
    "\n",
    "\n",
    "model.add(Reshape((1,1,100), input_shape=(1,1,1,100)))\n",
    "  # must reshape from a 3-D structure with 100 channels to a 2-D image having 100 channels\n",
    "  # so Conv2DTranspose can work properly\n",
    "\n",
    "\n",
    "model.add(Conv2DTranspose(filters=256,\n",
    "                          kernel_size=(2,2),\n",
    "                          strides=(1,1),\n",
    "                          padding='valid',\n",
    "                          use_bias=False\n",
    "                          ))\n",
    "model.add(BatchNormalization(center=True, scale=True))\n",
    "model.add(ReLU())\n",
    "\n",
    "model.add(Conv2DTranspose(filters=128,\n",
    "                          kernel_size=(4,4),\n",
    "                          strides=(2,2),\n",
    "                          padding='same',\n",
    "                          use_bias=False\n",
    "                          ))\n",
    "model.add(BatchNormalization(center=True, scale=True))\n",
    "model.add(ReLU())\n",
    "\n",
    "model.add(Conv2DTranspose(filters=64,\n",
    "                          kernel_size=(4,4),\n",
    "                          strides=(2,2),\n",
    "                          padding='same',\n",
    "                          use_bias=False\n",
    "                          ))\n",
    "model.add(BatchNormalization(center=True, scale=True))\n",
    "model.add(ReLU())\n",
    "\n",
    "model.add(Conv2DTranspose(filters=32,\n",
    "                          kernel_size=(4,4),\n",
    "                          strides=(2,2),\n",
    "                          padding='same',\n",
    "                          use_bias=False\n",
    "                          ))\n",
    "model.add(BatchNormalization(center=True, scale=True))\n",
    "model.add(ReLU())\n",
    "\n",
    "model.add(Conv2DTranspose(filters=1,\n",
    "                          kernel_size=(4,4),\n",
    "                          strides=(2,2),\n",
    "                          padding='same',\n",
    "                          use_bias=False\n",
    "                          ))\n",
    "model.add(Dense(1, activation='tanh'))\n",
    "\n",
    "# Variable LR for optimizer\n",
    "# MHS Note: I tried a decaying learning rate here, but you can take that out if you prefer\n",
    "lr_schedule = keras.optimizers.schedules.ExponentialDecay(\n",
    "    initial_learning_rate=1e-3,\n",
    "    decay_steps=100,\n",
    "    decay_rate=0.85)\n",
    "\n",
    "model.compile(loss='mean_squared_error',\n",
    "              optimizer=keras.optimizers.Adam(learning_rate=lr_schedule))\n",
    "print(\"input shape:\", sample_shape)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 13461,
     "status": "ok",
     "timestamp": 1615410911164,
     "user": {
      "displayName": "Chuan Chen",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjGNZ_BC0raiaLxFCSgR1dppBgMgjY4TPSe81Bz=s64",
      "userId": "01488084844237739692"
     },
     "user_tz": 300
    },
    "id": "g0YwgwP9qZdE",
    "outputId": "279eaf7a-0d71-4dec-8924-7fcf68423072",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Fit with DataGenerators\n",
    "model.fit(train_generator,\n",
    "          validation_data = valid_generator,\n",
    "          epochs = 200)\n",
    "\n",
    "# # Fit data to model\n",
    "# history = model.fit(X_train, y_train,\n",
    "#                     batch_size=1,\n",
    "#                     epochs=3,\n",
    "#                     verbose=1,\n",
    "#                     validation_split=0.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot the images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test,y_test = valid_generator[4] # Pick a random value (4 here) in valid_generator or train_generator\n",
    "y_pred = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Plots 5 samples from the chosen sample\n",
    "plt.figure(figsize=[6,15])\n",
    "for i in range(5):\n",
    "    plt.subplot(5,2,2*i+1)\n",
    "    if i==0:\n",
    "        plt.title(\"True\")\n",
    "    plt.imshow(y_test[i,:,:,0],cmap='gray')\n",
    "    plt.subplot(5,2,2*i+2)\n",
    "    plt.imshow(y_pred[i,:,:,0],cmap='gray')\n",
    "    if i==0:\n",
    "        plt.title(\"Predicted\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot the voxels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.gca(projection='3d')\n",
    "# ax.set_aspect('equal')\n",
    "\n",
    "ax.voxels(X_test[0,:,:,:,0], edgecolor=\"k\")\n",
    "ax.set_xlabel('X')\n",
    "ax.set_ylabel('Y')\n",
    "ax.set_zlabel('Z')\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyOedOz2PeSVnhOBnip0o1bu",
   "collapsed_sections": [],
   "name": "1_cnn on 3d voxel data.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
